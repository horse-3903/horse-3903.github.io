---
title: Language Modelling
published: 2026-02-01
description: "Modelling token sequences for generation and prediction."
tags: ["Natural Language Processing"]
category: IOAI ML Notes
draft: false
---

# Syllabus Map

* Study map: [Syllabus Study Map](/posts/syllabus/ioai-study-map/)

---
# Overview

* Language models learn to predict the next token in a sequence.

---

# Autoregressive Modelling

## Core Idea

* Predict token t given previous tokens.

## Practical Notes

* Enables text generation.
* Often trained with large corpora.

---

# Masked Modelling

## Core Idea

* Predict masked tokens using full context.

## Practical Notes

* Used by encoder models like BERT.




