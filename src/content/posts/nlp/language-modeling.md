---
title: Language Modeling
published: 2026-02-01
description: "Modeling token sequences for generation and prediction."
tags: ["NLP", "Language Models"]
category: Notes
draft: false
---

# Syllabus Map

* Study map: [Syllabus Study Map](/posts/syllabus/ioai-study-map/)

---
# Overview

* Language models learn to predict the next token in a sequence.

---

# Autoregressive Modeling

## Core Idea

* Predict token t given previous tokens.

## Practical Notes

* Enables text generation.
* Often trained with large corpora.

---

# Masked Modeling

## Core Idea

* Predict masked tokens using full context.

## Practical Notes

* Used by encoder models like BERT.



