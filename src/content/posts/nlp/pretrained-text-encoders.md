# SMU H3 Map

* Content map: [SMU H3 Game Theory Map](/posts/game-theory/smu-h3/)

---

ï»¿---
title: Pre-trained Text Encoders
published: 2026-02-01
description: "Transformer-based text encoders and their use cases."
tags: ["NLP", "Transformers"]
category: Notes
draft: false
---

# Syllabus Map

* Study map: [Syllabus Study Map](/posts/syllabus/study-map/)

---
# Overview

* Pre-trained encoders provide rich text representations.

---

# Example Models

* BERT.
* RoBERTa.
* DistilBERT.

---

# Common Uses

## Feature Extraction

* Freeze encoder and train a small head.

## Fine-Tuning

* Update all weights for task-specific performance.


