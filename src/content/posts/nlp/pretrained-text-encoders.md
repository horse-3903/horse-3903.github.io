---
title: Pre-trained Text Encoders
published: 2026-02-01
description: "Transformer-based text encoders and their use cases."
tags: ["Natural Language Processing"]
category: Notes
draft: false
---

# Syllabus Map

* Study map: [Syllabus Study Map](/posts/syllabus/ioai-study-map/)

---
# Overview

* Pre-trained encoders provide rich text representations.

---

# Example Models

* BERT.
* RoBERTa.
* DistilBERT.

---

# Common Uses

## Feature Extraction

* Freeze encoder and train a small head.

## Fine-Tuning

* Update all weights for task-specific performance.



