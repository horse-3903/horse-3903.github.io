---
title: Transformers
published: 2026-02-01
description: "Transformer architecture for sequence modeling."
tags: ["Natural Language Processing", "Deep Learning"]
category: Notes
draft: false
---

# Syllabus Map

* Study map: [Syllabus Study Map](/posts/syllabus/ioai-study-map/)

---
# Overview

* Transformers use attention to model sequences.
* They enable parallelized training and long-range dependencies.

---

# Core Components

## Self-Attention

* Computes weighted combinations of tokens.

## Multi-Head Attention

* Captures different relational patterns.

## Positional Encoding

* Injects order information.

---

# Practical Notes

* Scales well with data and compute.
* Foundation for modern LLMs.



