---
title: Probability Fundamentals for ML
published: 2026-02-16
description: "Core probability and statistics concepts needed to understand IOAI ML topics."
tags: ["Mathematics Fundamentals", "Probability"]
category: IOAI ML Notes
draft: false
access: public
---
# Syllabus Map

* Study map: [Syllabus Study Map](/posts/syllabus/ioai-study-map/)

---


# Required Concepts

## Random Variables, Expectation, and Variance

* Distribution of outcomes
* Mean as central tendency
* Variance as spread/uncertainty

## Conditional Probability and Bayes Rule

* Updating beliefs with evidence
* Posterior reasoning

## Common Distributions

* Bernoulli/Binomial for binary/count outcomes
* Gaussian for continuous noise and aggregate effects

## Likelihood and MLE

* Parameter fitting by maximizing observed-data likelihood
* Log-likelihood and numerical optimization

## Entropy, Cross-Entropy, and KL Divergence

* Uncertainty and information mismatch measures
* Core losses for classification and representation learning

## Bias-Variance Tradeoff

* Decomposition of prediction error
* Underfitting vs overfitting behavior

## Sampling and Data Splits

* Train/validation/test protocol
* Data leakage awareness

## Correlation vs Independence

* Linear association vs full probabilistic independence
* Common interpretation pitfalls

## Basic Statistical Inference

* Confidence intervals and hypothesis testing intuition
* Interpreting model comparisons with uncertainty

---

## Why This Matters for ML

* These concepts provide the uncertainty framework behind model fitting, evaluation, and generalization.
* They are essential for principled loss design and trustworthy ML conclusions.